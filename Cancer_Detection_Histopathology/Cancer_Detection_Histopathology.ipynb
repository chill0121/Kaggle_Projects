{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Detection Histopathology Using Deep Learning\n",
    "\n",
    "This project is will take Whole Slide Imaging (WSI) patches of tissue in combination with Convolutional Neural Networks to classify metastatic cancer.\n",
    "\n",
    "---\n",
    "\n",
    "This project derives from the Kaggle competition with the name Histopathologic Cancer Detection, found here: https://www.kaggle.com/competitions/histopathologic-cancer-detection/overview\n",
    "\n",
    "You can find this project at the github repo: https://github.com/chill0121/Kaggle_Projects/tree/main/Cancer_Detection_Histopathology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a name=\"toc\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "- 1.[**Data Source Information**](#datasource)\n",
    "  - 1.1. [Dataset Information](#data)\n",
    "  - 1.2. [Kaggle Information](#kaggle)\n",
    "- 2.[**Setup**](#setup)\n",
    "  - 2.1. [Environment Details for Reproducility](#env)\n",
    "  - 2.2. [Importing the Data](#dataimport)\n",
    "- 3.[**Data Preprocessing**](#datapre)\n",
    "  - 3.1. [First Looks](#firstlook)\n",
    "  - 3.2. [Missing Data](#missingdata)\n",
    "  - 3.3. [Data Cleanup](#dataclean)\n",
    "  - 3.4. [Checking for Duplicate Entries](#duplicates)\n",
    "- 4.[**Exploratory Data Analysis (EDA)**](#eda)\n",
    "- 5.[**Models**](#models)\n",
    "  - 5.1. [Baseline Models](#baseline)\n",
    "  - 5.2. [Model Helper Functions](#helper)\n",
    "  - 5.3. [Deep Learning Models](#deep)\n",
    "- 6.[**Results**](#results)\n",
    "- 7.[**Conclusion - Kaggle Submission Test Set**](#conclusion)\n",
    "  - 7.1. [Possible Areas for Improvement](#improvements)\n",
    "\n",
    "- [**Appendix A - Online References**](#appendixa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Source Information <a name=\"datasource\"></a>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data Information: <a name=\"data\"></a>\n",
    "\n",
    "The data in this project is an altered/reduced version of PatchCamelyon (PCam). It consists of color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue. This dataset was first introduced in this paper: https://arxiv.org/abs/1806.03962v1\n",
    "\n",
    "\"In this dataset, you are provided with a large number of small pathology images to classify. Files are named with an image id. The train_labels.csv file provides the ground truth for the images in the train folder. You are predicting the labels for the images in the test folder. A positive label indicates that the center 32x32px region of a patch contains at least one pixel of tumor tissue. Tumor tissue in the outer region of the patch does not influence the label. This outer region is provided to enable fully-convolutional models that do not use zero-padding, to ensure consistent behavior when applied to a whole-slide image.\n",
    "\n",
    "The original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates. We have otherwise maintained the same data and splits as the PCam benchmark.\"\n",
    "\n",
    "**Data Info:**\n",
    "- 277,485 Slide Image Patches\n",
    "    - Images: 96 x 96 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Kaggle Information: <a name=\"kaggle\"></a>\n",
    "\n",
    "#### Description:\n",
    "\n",
    "In this competition, you must create an algorithm to identify metastatic cancer in small image patches taken from larger digital pathology scans. The data for this competition is a slightly modified version of the PatchCamelyon (PCam) benchmark dataset (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).\n",
    "\n",
    "#### Evaluation:\n",
    "\n",
    "Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n",
    "\n",
    "#### Citation: \n",
    "\n",
    "Will Cukierski. (2018). Histopathologic Cancer Detection. Kaggle. https://kaggle.com/competitions/histopathologic-cancer-detection\n",
    "\n",
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup <a name=\"setup\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score,classification_report, confusion_matrix, ConfusionMatrixDisplay, auc, roc_curve, RocCurveDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Environment Information for Reproducibility: <a name=\"env\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "<module 'pandas'  using version: 2.1.4\n",
      "<module 'numpy'  using version: 1.26.4\n",
      "<module 'seaborn'  using version: 0.13.2\n",
      "<module 'sklearn'  using version: 1.3.2\n",
      "<module 'tensorflow'  using version: 2.16.2\n",
      "<module 'torch'  using version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "packages = [pd, np, sns, sklearn, tf, torch]\n",
    "for package in packages:\n",
    "    print(f\"{str(package).partition('from')[0]} using version: {package.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Importing the Data: <a name=\"dataimport\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "current_wdir = os.getcwd()\n",
    "data_folder = current_wdir + '/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_import(file_list, folder_name):\n",
    "    '''\n",
    "    Takes a list of image filenames and loads the images into a dictionary with the same filename(*).\n",
    "    \n",
    "    Parameters:\n",
    "        file_list: List of filenames in the form ['name.extension', ...]\n",
    "        folder_name: String of folder name within ./Data/ to import.\n",
    "    Returns:\n",
    "        image_array: ndarray of images.\n",
    "        id_array: ndarray of image ids.\n",
    "    '''\n",
    "    id_array = np.empty(len(file_list), dtype = object)\n",
    "    image_array = np.zeros((len(file_list), 96, 96, 3)) # 4D Array with shape (n_images, height, width, channels)\n",
    "\n",
    "    for i, file in enumerate(file_list):\n",
    "        # Separate file extension and image id.\n",
    "        id, _ = file.split('.')\n",
    "        img = Image.open(f'./Data/{folder_name}/{file}')\n",
    "        id_array[i] = id\n",
    "        image_array[i] = np.asarray(img)\n",
    "        if i % 10_000 == 0:\n",
    "            print(f'..importing image # {i}')\n",
    "\n",
    "    return image_array, id_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..importing image #0\n",
      "..importing image #10000\n",
      "..importing image #20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m files_train \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Data/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m files_test \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Data/test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train_images, train_ids \u001b[38;5;241m=\u001b[39m \u001b[43mimage_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_images, test_ids \u001b[38;5;241m=\u001b[39m image_import(files_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 18\u001b[0m, in \u001b[0;36mimage_import\u001b[0;34m(file_list, folder_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(file_list):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Separate file extension and image id.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mid\u001b[39m, _ \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     id_array[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m\n\u001b[1;32m     20\u001b[0m     image_array[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(img)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files_train = os.listdir('./Data/train')\n",
    "files_test = os.listdir('./Data/test')\n",
    "\n",
    "train_images, train_ids = image_import(files_train, 'train')\n",
    "test_images, test_ids = image_import(files_test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/chill/GitHub/Kaggle_Projects/Cancer_Detection_Histopathology/Data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add and sort all filenames from each folder path.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[1;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(file_path)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate through filenames and add them to dataframe.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/chill/GitHub/Kaggle_Projects/Cancer_Detection_Histopathology/Data/'"
     ]
    }
   ],
   "source": [
    "# Add and sort all filenames from each folder path.\n",
    "file_path = [f'{data_folder}/{file}' for file in os.listdir(data_folder) if '.csv' in file]\n",
    "file_path = sorted(file_path)\n",
    "\n",
    "# Iterate through filenames and add them to dataframe.\n",
    "train = pd.read_csv(data_folder + '/train.csv')\n",
    "X_test = pd.read_csv(data_folder + '/test.csv')\n",
    "sample_y_test = pd.read_csv(data_folder + '/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing <a name=\"datapre\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. First Looks: <a name=\"firstlook\"></a>\n",
    "\n",
    "Print out some basic information about the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Train\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           int64\n",
      "keyword     object\n",
      "location    object\n",
      "text        object\n",
      "target       int64\n",
      "dtype: object\n",
      "\n",
      "------------------------------------\n",
      "Test\n",
      "------------------------------------\n",
      "['id', 'keyword', 'location', 'text']\n",
      "(3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------\\nTrain\\n-------------------------------------')\n",
    "display(train)\n",
    "print(train.dtypes)\n",
    "print('\\n------------------------------------\\nTest\\n------------------------------------')\n",
    "print(X_test.columns.to_list())\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep all future plots clear and consistent, a color map dictionary will map the classes to a color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build custom color map for consistent label visualization.\n",
    "class_cmap = {'Not Disaster' : '#012A36',\n",
    "              'Disaster' : '#D16666'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Missing Data: <a name=\"missingdata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should ensure there aren't any tweets with completely missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Text Cleanup: <a name=\"textclean\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Checking for Duplicate Entries: <a name=\"duplicates\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to ensure there aren't any duplicate entries in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates Found: 61\n",
      "DF Shape: (7613, 12)\n",
      "Duplicates Found: 0\n",
      "DF Shape: (7552, 12)\n"
     ]
    }
   ],
   "source": [
    "# Training Set\n",
    "print('Duplicates Found:', train.text.duplicated().sum())\n",
    "print('DF Shape:', train.shape)\n",
    "\n",
    "drop_idx = train[train.text.duplicated()].index\n",
    "train = train.drop(drop_idx, axis = 0)\n",
    "\n",
    "print('Duplicates Found:', train.text.duplicated().sum())\n",
    "print('DF Shape:', train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "110 tweets showed up as being duplicates in the training data. These have now been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates Found: 20\n",
      "DF Shape: (3263, 10)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print('Duplicates Found:', X_test.text.duplicated().sum())\n",
    "print('DF Shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are also 20 duplicate tweets in the testing set. Since the testing set was created for us and to be submitted into Kaggle for scoring, we can't fix this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA) <a name=\"eda\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Models and Embedding <a name=\"models\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Baseline Models <a name=\"baseline\"></a>\n",
    "\n",
    "It's always important to set a suitable baseline for comparison.\n",
    "\n",
    "The first baseline model is simple, equal random chance at selecting any of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Baseline F1-Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "mod_rand_baseline = 1 / len(train.target.unique()) # 1/2\n",
    "print('Random Baseline F1-Score:', mod_rand_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next option is taking the most frequent class in the dataset and always predict that class.\n",
    "\n",
    "*Note: The y predictions will be submitted to Kaggle to receive the accuracy score and posted here and in the results section.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       0\n",
       "2         3       0\n",
       "3         9       0\n",
       "4        11       0\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       0\n",
       "3260  10868       0\n",
       "3261  10874       0\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Category Baseline F1-Score: 0.57033\n"
     ]
    }
   ],
   "source": [
    "# most_freq_cat = train.target.value_counts(sort = True).index[0]\n",
    "# mod_freq_array = np.full(shape = len(X_test), fill_value = most_freq_cat)\n",
    "\n",
    "# y_pred_freq_baseline = X_test[['id']].copy()\n",
    "# y_pred_freq_baseline['target'] = mod_freq_array\n",
    "# y_pred_freq_baseline.to_csv(current_wdir + f'/Models/Frequency_Baseline/X_test_Submission_Freq_Baseline.csv', index = False)\n",
    "# display(y_pred_freq_baseline)\n",
    "\n",
    "# mod_freq_baseline = 0.57033\n",
    "# print('Most Frequent Category Baseline F1-Score:', mod_freq_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Model Helper Functions <a name=\"helper\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have created a few functions to help visualize the training process, tracking and plotting the training and validation metric and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_region_overlay(model_history_df, x_offset):\n",
    "    x_mid = ((model_history_df.index.stop-1) + model_history_df.val_loss.idxmin()) / 2\n",
    "    plt.text(x = x_mid - x_offset,\n",
    "             y = (plt.ylim()[0] + plt.ylim()[1]) / 2,\n",
    "             s = 'Early Stop',\n",
    "             rotation = 'horizontal',\n",
    "             weight = 'extra bold',\n",
    "             fontsize = 'large',\n",
    "             antialiased = True,\n",
    "             alpha = 1,\n",
    "             c = 'white',\n",
    "             bbox = dict(facecolor = 'black', edgecolor = 'black', boxstyle = 'round', alpha = 0.5))\n",
    "    return None\n",
    "\n",
    "def plot_TF_training_history(model_history_df):\n",
    "\n",
    "    # Find all epochs that callback ReduceLROnPlateau() occurred.\n",
    "    lr_change = model_history_df.learning_rate.shift(-1) != model_history_df.learning_rate\n",
    "\n",
    "    # Create color map and lines style map for train/val\n",
    "    plot_maps = {'cmap': {'accuracy': '#653096',\n",
    "                        'loss': '#653096',\n",
    "                        'val_accuracy': '#004a54',\n",
    "                        'val_loss': '#004a54'},\n",
    "                'dashmap': {'accuracy': '',\n",
    "                            'loss': (2,1),\n",
    "                            'val_accuracy': '',\n",
    "                            'val_loss': (2,1)}}\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize = (10,6))\n",
    "    ax = sns.lineplot(model_history_df.drop(columns = ['learning_rate']).iloc[1:], palette = plot_maps['cmap'], dashes = plot_maps['dashmap'])\n",
    "    ax.set_xlabel('Epoch')\n",
    "\n",
    "    # Create secondary x-axis for Learning Rate changes.\n",
    "    sec_ax = ax.secondary_xaxis('top')\n",
    "    sec_ax.set_xticks(model_history_df[lr_change].index[:-1])\n",
    "    sec_ax.set_xticklabels([f'{x:.1e}' for x in model_history_df[lr_change].learning_rate[1:]])\n",
    "    sec_ax.tick_params(axis = 'x', which = 'major', labelsize = 7)\n",
    "    sec_ax.set_xlabel('Learning Rate Reductions')\n",
    "\n",
    "    # Create vertical line for each LR change.\n",
    "    for epoch in (model_history_df[lr_change].index[:-1]):\n",
    "        plt.axvline(x = epoch, c = '#d439ad', ls = (0, (5,5)))\n",
    "    # Create lines for best epoch/val_loss.\n",
    "    plt.axvline(x = (model_history_df.val_loss.idxmin()), c = '#f54260', ls = (0, (3,1,1,1)))\n",
    "    plt.axhline(y = (model_history_df.val_loss.min()), c = '#f54260', alpha = 0.3, ls = (0, (3,1,1,1)))\n",
    "    # Grey out epochs after early stop.\n",
    "    plt.axvspan(model_history_df.val_loss.idxmin(), model_history_df.index.stop-1, facecolor = 'black', alpha = 0.25)\n",
    "    plt.margins(x = 0)\n",
    "    set_region_overlay(model_history_df, 5)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Deep Learning <a name=\"deep\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results <a name=\"results\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the best model's test results green at each proportion.\n",
    "def max_value_highlight(df):\n",
    "    max_test_rows = df.max()\n",
    "    is_max = (df == max_test_rows)\n",
    "    \n",
    "    return ['background-color:green' if v else '' for v in is_max]\n",
    "\n",
    "# Highlight the top two results in each column blue so that 2nd place is in blue after .apply().\n",
    "def highlight_top_two(df):\n",
    "    # Sort values\n",
    "    test_rows = df\n",
    "    sorted_df = test_rows.sort_values(ascending = False)\n",
    "    top_two = sorted_df.iloc[: 2]\n",
    "    # Mask\n",
    "    is_top_two = df.isin(top_two)\n",
    "\n",
    "    return ['background-color: blue' if v else '' for v in is_top_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the test set the .csv files must be submitted to Kaggle. Each model's predictions were saved above and manually submitted. Below you can find a screenshot of all the results.\n",
    "\n",
    "<img src=\"https://github.com/chill0121/Kaggle_Projects/blob/main/NLP_Disaster_Tweets/Models/Kaggle_Results.png?raw=true\" alt=\"results\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_64506_row3_col0 {\n",
       "  background-color: blue;\n",
       "  background-color: green;\n",
       "}\n",
       "#T_64506_row4_col0 {\n",
       "  background-color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_64506\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_64506_level0_col0\" class=\"col_heading level0 col0\" >F1-Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_64506_level0_row0\" class=\"row_heading level0 row0\" >Random_Baseline</th>\n",
       "      <td id=\"T_64506_row0_col0\" class=\"data row0 col0\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64506_level0_row1\" class=\"row_heading level0 row1\" >Frequent_Baseline</th>\n",
       "      <td id=\"T_64506_row1_col0\" class=\"data row1 col0\" >0.570330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64506_level0_row2\" class=\"row_heading level0 row2\" >RNN</th>\n",
       "      <td id=\"T_64506_row2_col0\" class=\"data row2 col0\" >0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64506_level0_row3\" class=\"row_heading level0 row3\" >LSTM</th>\n",
       "      <td id=\"T_64506_row3_col0\" class=\"data row3 col0\" >0.779030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64506_level0_row4\" class=\"row_heading level0 row4\" >GRU</th>\n",
       "      <td id=\"T_64506_row4_col0\" class=\"data row4 col0\" >0.735820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x33d77e450>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Kaggle Submission Scores for Test Set.\n",
    "# results_test = {'Random_Baseline' : mod_rand_baseline,\n",
    "#                 'Frequent_Baseline' : mod_freq_baseline,\n",
    "#                 'RNN' : 0.73000,\n",
    "#                 'LSTM' : 0.77903,\n",
    "#                 'GRU' : 0.73582}\n",
    "\n",
    "# results_test_df = pd.DataFrame().from_dict(results_test, orient = 'index', columns = ['F1-Score'])\n",
    "# results_test_df.index.name = 'Model'\n",
    "\n",
    "# results_test_df.style.apply(highlight_top_two).apply(max_value_highlight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model's score is highlighted in green and the 2nd best is in blue.\n",
    "\n",
    "Discussions can be found in the conclusion section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion - Kaggle Submission Test Set <a name=\"conclusion\"></a>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Possible Areas for Improvement <a name=\"improvements\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A - Online References: <a name=\"appendixa\"></a>\n",
    "\n",
    "Resources that helped along the way in no particular order.\n",
    "\n",
    "1. \n",
    "\n",
    " Exported to HTML via command line using:\n",
    "\n",
    "- `jupyter nbconvert NLP_Disaster_Tweets.ipynb --to html`\n",
    "- `jupyter nbconvert NLP_Disaster_Tweets.ipynb --to html --HTMLExporter.theme=dark`\n",
    "\n",
    "###### [Back to Table of Contents](#toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
